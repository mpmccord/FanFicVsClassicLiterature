{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjz-4n_vDMl9"
   },
   "source": [
    "# Preprocessing text with Spark NPL - Overview\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br3PXqMvDfy-"
   },
   "source": [
    "## 1. Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "teXvoXAvDZY0",
    "outputId": "a98b20d0-169a-4fde-9f60-53f35ce74cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "openjdk version \"1.8.0_312\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~20.04-b07)\n",
      "OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "!apt-get update -qq\n",
    "!apt-get install -y openjdk-8-jdk-headless -qq\n",
    "\n",
    "#Install Java\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TcGxv3YuDnx4",
    "outputId": "6d63fcd8-3650-427c-aed2-91ed7ee2bb22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "Collecting pyspark==2.4.4\n",
      "  Using cached pyspark-2.4.4-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.7\n",
      "  Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "Collecting spark-nlp==2.6.2\n",
      "  Using cached spark_nlp-2.6.2-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "Installing collected packages: spark-nlp\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/opt/conda/lib/python3.8/site-packages/com/__init__.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -python (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install Pyspark\n",
    "! pip install --ignore-installed pyspark==2.4.4\n",
    "\n",
    "#Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==2.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py2OyUQODs-S"
   },
   "source": [
    "## 2. Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6T94RjuTDzGz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 18:40:32 WARN Utils: Your hostname, DSGPU05 resolves to a loopback address: 127.0.1.1; using 10.10.11.64 instead (on interface eno1)\n",
      "22/04/12 18:40:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mmccord/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mmccord/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-befd48c5-918c-4946-89cb-1ead110cd517;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;3.4.2 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.5.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.603 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.json4s#json4s-ext_2.12;3.5.3 in central\n",
      "\tfound joda-time#joda-time;2.9.5 in central\n",
      "\tfound org.joda#joda-convert;1.8.1 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 in central\n",
      "\tfound net.sf.trove4j#trove4j;3.0.3 in central\n",
      ":: resolution report :: resolve 493ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.603 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;3.4.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjoda-time#joda-time;2.9.5 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\tnet.sf.trove4j#trove4j;3.0.3 from central in [default]\n",
      "\torg.joda#joda-convert;1.8.1 from central in [default]\n",
      "\torg.json4s#json4s-ext_2.12;3.5.3 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.5.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   0   ||   21  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-befd48c5-918c-4946-89cb-1ead110cd517\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 21 already retrieved (0kB/10ms)\n",
      "22/04/12 18:40:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/12 18:40:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/04/12 18:40:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start()\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aJzJRbsD379"
   },
   "source": [
    "## 3. Load and Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fW9IpYkLD9FP"
   },
   "source": [
    "In this part, I am reading **.csv files** that I generated from the **classics** .txt files.\n",
    "These .csv files were **generated in python**, exporting the final dataframes we already built to csv.\n",
    "I am sharing these csv files **in GIT** (https://github.com/mpmccord/FanFicVsClassicLiterature/tree/main/data). \n",
    "\n",
    "There are 4 of them:\n",
    "\n",
    "\n",
    "1.   **classics_clean.csv:** The eight classics with the whole corpus, preprocessed (lower case, remove spaces, etc.)\n",
    "2.   **classics_raw.csv:**  The eight classics with the whole corpus, raw \n",
    "3.   **classics_clean_test.csv:** Subset of cleaned classics. Just two of them with only 200 words of the corpus.\n",
    "4.   **classics_raw_test.csv:** Subset of raw classics. Just two of them with only 200 words of the corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ7LXjJbFYbf"
   },
   "source": [
    "### 3.1 Read classics_raw_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flQL5EQ7FjW3"
   },
   "source": [
    "For testing porpuses and for short computational times, I am going to use this subset with only two incomplete raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIZ41P5iFyea",
    "outputId": "b65421dd-a742-4283-afe3-fca6b810011a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                                      _c0|                   text|                                                                                                         id|                                                                                                type|\n",
      "+-----------------------------------------+-----------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                                        0|              Chapter 1|                                                                                                       null|                                                                                                null|\n",
      "|Going back was the worst.I had hoped that| after a case like mine| Iâ€™d be moved to another school. No such luck. My trip in the locker had no positive consequence whatsoever| and I was back in school after more than a month outside of it.I forced my feet one after the other|\n",
      "+-----------------------------------------+-----------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 18:41:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , text, id, type\n",
      " Schema: _c0, text, id, type\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/mmccord/Distributed_Computing/FanFicVsClassicLiterature/data/fanfics_raw.csv\n"
     ]
    }
   ],
   "source": [
    "#Generate Spark dataframe from csv file\n",
    "df_Spark = spark.read \\\n",
    "           .option(\"header\", True) \\\n",
    "           .csv(\"data/fanfics_raw.csv\") #Change path accordingly to yours\n",
    "\n",
    "df_Spark.show(2, truncate=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9m3hBQZF8SN"
   },
   "source": [
    "## 4. Create Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYzZHjoXGBs3"
   },
   "source": [
    "Create pipeline to preprocess the spark dataframe texts.\n",
    "\n",
    "Each of these classes receive an input column and creates the output column.\n",
    "At the end of the pipeline, we will have a dataframe with all of the columns that are created on the fly and their results.\n",
    "\n",
    "The **last column** generated, in this case **token_features** is the one that has all the words after being preprocessed, removing stop words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRf9LnvgGQwi",
    "outputId": "f64baa8b-1b6f-4571-c018-a71e48d44d9d"
   },
   "outputs": [],
   "source": [
    "#https://medium.com/spark-nlp/spark-nlp-101-document-assembler-500018f5f6b5\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink_full\") #remove new lines and tabs, plus shrinking spaces and blank lines.\n",
    "\n",
    "#We dont need this because when preprocessing, the test ends up being one sentence\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "#https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.Tokenizer.html\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "#https://nlp.johnsnowlabs.com/docs/en/annotators\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^A-Za-z]\"\"\"]) # remove punctuations and alphanumeric chars\n",
    "\n",
    "#Stop words used by Spark NLP: http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n",
    "#https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/b2eb08610dd49d5b15077cc499a94b4ec1e8b861/jupyter/annotation/english/stop-words/StopWordsCleaner.ipynb#scrollTo=1-eGocORg2ml\n",
    "stop_words = StopWordsCleaner.pretrained('stopwords_en', 'en')\\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"cleanTokens\") \\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "#https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/LemmatizerModel.html\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "         .setInputCols([\"cleanTokens\"]) \\\n",
    "         .setOutputCol(\"lemma\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCols([\"token_features\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfGrTrtZGVtN"
   },
   "outputs": [],
   "source": [
    "nlp_pipeline_lr = Pipeline(\n",
    "        stages=[document, \n",
    "                sentence,\n",
    "                token,\n",
    "                normalizer,\n",
    "                stop_words, \n",
    "                lemmatizer, \n",
    "                finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVRtMFSvGZ7b"
   },
   "source": [
    "## 5. Apply Pipeline to Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf-t11ocGZhx"
   },
   "outputs": [],
   "source": [
    "processed_text = nlp_pipeline_lr.fit(df_Spark).transform(df_Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLxlq30UGjg3",
    "outputId": "c93a4104-409e-4f7b-b584-3fc3c5155efc"
   },
   "outputs": [],
   "source": [
    "processed_text.show(truncate=200) #Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2REFQsLHrgY",
    "outputId": "22f7c1e5-ce13-4bd9-99a0-2df160a103e0"
   },
   "outputs": [],
   "source": [
    "#Show last column, the one that has the final result\n",
    "processed_text.select(\"token_features\").show(truncate=200) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Preprocessing text with Spark NPL-Overview.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

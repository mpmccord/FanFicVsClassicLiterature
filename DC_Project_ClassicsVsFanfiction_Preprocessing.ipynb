{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjz-4n_vDMl9"
   },
   "source": [
    "# Distributed Computing - Project 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br3PXqMvDfy-"
   },
   "source": [
    "## 1.Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7mjzu90Ybqw"
   },
   "source": [
    "### 1.1 Install Java, Pyspark and Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "teXvoXAvDZY0",
    "outputId": "f4dc801d-7b3d-4363-bdd6-3bd0cc7cf940"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update -qq\n",
    "#!apt-get install -y openjdk-8-jdk-headless -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_312\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~20.04-b07)\n",
      "OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "#Install Java\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TcGxv3YuDnx4",
    "outputId": "2555c265-1675-4ce2-9c41-c655466c0ed5"
   },
   "outputs": [],
   "source": [
    "#Install Pyspark\n",
    "#! pip install --ignore-installed pyspark==2.4.4\n",
    "\n",
    "#Install Spark NLP\n",
    "#! pip install --ignore-installed spark-nlp==2.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp2beC4FYesx"
   },
   "source": [
    "### 1.2 Start  Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TGDRKTctUzRF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 17:47:19 WARN Utils: Your hostname, DSGPU05 resolves to a loopback address: 127.0.1.1; using 10.10.11.64 instead (on interface eno1)\n",
      "22/04/12 17:47:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mmccord/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mmccord/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp-spark32_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a1607889-85cb-4325-8458-1397887bf5f6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp-spark32_2.12;3.4.2 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.5.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.603 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.json4s#json4s-ext_2.12;3.7.0-M11 in central\n",
      "\tfound joda-time#joda-time;2.10.10 in central\n",
      "\tfound org.joda#joda-convert;2.2.1 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 in central\n",
      "\tfound net.sf.trove4j#trove4j;3.0.3 in central\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/spark-nlp-spark32_2.12/3.4.2/spark-nlp-spark32_2.12-3.4.2.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#spark-nlp-spark32_2.12;3.4.2!spark-nlp-spark32_2.12.jar (2374ms)\n",
      "downloading https://repo1.maven.org/maven2/org/json4s/json4s-ext_2.12/3.7.0-M11/json4s-ext_2.12-3.7.0-M11.jar ...\n",
      "\t[SUCCESSFUL ] org.json4s#json4s-ext_2.12;3.7.0-M11!json4s-ext_2.12.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.10.10/joda-time-2.10.10.jar ...\n",
      "\t[SUCCESSFUL ] joda-time#joda-time;2.10.10!joda-time.jar (51ms)\n",
      "downloading https://repo1.maven.org/maven2/org/joda/joda-convert/2.2.1/joda-convert-2.2.1.jar ...\n",
      "\t[SUCCESSFUL ] org.joda#joda-convert;2.2.1!joda-convert.jar (22ms)\n",
      ":: resolution report :: resolve 2420ms :: artifacts dl 2492ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.603 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp-spark32_2.12;3.4.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjoda-time#joda-time;2.10.10 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\tnet.sf.trove4j#trove4j;3.0.3 from central in [default]\n",
      "\torg.joda#joda-convert;2.2.1 from central in [default]\n",
      "\torg.json4s#json4s-ext_2.12;3.7.0-M11 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.5.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   4   |   4   |   0   ||   21  |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a1607889-85cb-4325-8458-1397887bf5f6\n",
      "\tconfs: [default]\n",
      "\t4 artifacts copied, 17 already retrieved (40227kB/57ms)\n",
      "22/04/12 17:47:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "#spark = sparknlp.start()\n",
    "spark = sparknlp.start(spark32=True)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTvaEXcQU2pP"
   },
   "source": [
    "## 2.Get Classics Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB-99X0IVCsH"
   },
   "source": [
    "### 2.1 Convert txt files into Python Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1jCVlB2AVt1c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ybHiAU2Vwz0"
   },
   "outputs": [],
   "source": [
    "#directory =\"/content/drive/MyDrive/Distributed-Computing/data/classic_literature/\" #Change according to path\n",
    "directory =\"data/classic_literature/\" #Change according to path\n",
    "text_type = 'C'\n",
    "\n",
    "classics_df = pd.DataFrame(columns=['id', 'type', 'text'])\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "#filename = \"data/classic_literature/45.txt\"\n",
    "    file_ext = os.path.basename(filename).rsplit('.',1)[1] #Get file extension\n",
    "    if file_ext == \"txt\":\n",
    "        with open(directory + '/' + filename, 'r') as file:\n",
    "            text_id = os.path.basename(filename).rsplit('.',1)[0]\n",
    "            corpus = file.read()\n",
    "            corpus = re.sub(';', ' ', corpus)\n",
    "            corpus = corpus.replace('Chapter', '')\n",
    "            classics_df.loc[len(classics_df.index)] = [text_id, text_type, corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "hMUL6ZGUV3U5",
    "outputId": "164d8c67-900f-47fd-cdbd-6a6d01fbe87b"
   },
   "outputs": [],
   "source": [
    "classics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4O7Ft9_VVGZ"
   },
   "source": [
    "### 2.2 Convert Python Dataframe into Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSBwrf4sVnOT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#sc =  pyspark.SparkContext(\"local[*]\", \"Test Context\")\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puETLHbXWG-J"
   },
   "outputs": [],
   "source": [
    "df_Spark_classics = sqlContext.createDataFrame(classics_df) #Pyspark SQL dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATRk8V1dXcjR"
   },
   "outputs": [],
   "source": [
    "#df_Spark_classics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tnqsgNLVqcr"
   },
   "source": [
    "## 3.Get Fanfictions Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9AwiLtTX9Gf"
   },
   "source": [
    "### 3.1 Convert txt files into Python Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_061UaIX8Wl"
   },
   "outputs": [],
   "source": [
    "#directory =\"/content/drive/MyDrive/Distributed-Computing/data/fanfiction/\" #Change according to path\n",
    "directory =\"data/fanfiction/\" #Change according to path\n",
    "text_type = 'F'\n",
    "\n",
    "fanfictions_df = pd.DataFrame(columns=['id', 'type', 'text'])\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_ext = os.path.basename(filename).rsplit('.',1)[1] #Get file extension\n",
    "    if file_ext == \"txt\":\n",
    "        with open(directory + '/' + filename, 'r') as file:\n",
    "            text_id = os.path.basename(filename).rsplit('.',1)[0]\n",
    "            corpus = file.read()\n",
    "            corpus = corpus.replace('Chapter', '')\n",
    "            corpus = re.sub(';', ' ', corpus)\n",
    "            fanfictions_df.loc[len(fanfictions_df.index)] = [text_id, text_type, corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "iZ2UUw7D7QKt",
    "outputId": "819eb55a-1b3d-41b9-a469-decf732962e8"
   },
   "outputs": [],
   "source": [
    "fanfictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_xfXM-JYCVe"
   },
   "source": [
    "### 3.2 Convert Python Dataframe into Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2nGH71mX71g"
   },
   "outputs": [],
   "source": [
    "df_Spark_fanfictions = sqlContext.createDataFrame(fanfictions_df) #Pyspark SQL dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "675Exsu0Y3uL"
   },
   "source": [
    "## 4 Preprocess Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9m3hBQZF8SN"
   },
   "source": [
    "### 4.1 Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYzZHjoXGBs3"
   },
   "source": [
    "Create pipeline to preprocess the spark dataframe texts.\n",
    "\n",
    "Each of these classes receive an input column and creates the output column.\n",
    "At the end of the pipeline, we will have a dataframe with all of the columns that are created on the fly and their results.\n",
    "\n",
    "The **last column** generated, in this case **token_features** is the one that has all the words after being preprocessed, removing stop words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRf9LnvgGQwi",
    "outputId": "17982f91-18cb-48db-a01b-90829528f1c4"
   },
   "outputs": [],
   "source": [
    "#https://medium.com/spark-nlp/spark-nlp-101-document-assembler-500018f5f6b5\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink_full\") #remove new lines and tabs, plus shrinking spaces and blank lines.\n",
    "\n",
    "#We dont need this because when preprocessing, the test ends up being one sentence\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "#https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.Tokenizer.html\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "#https://nlp.johnsnowlabs.com/docs/en/annotators\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^A-Za-z]\"\"\"]) # remove punctuations and alphanumeric chars\n",
    "\n",
    "#Stop words used by Spark NLP: http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n",
    "#https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/b2eb08610dd49d5b15077cc499a94b4ec1e8b861/jupyter/annotation/english/stop-words/StopWordsCleaner.ipynb#scrollTo=1-eGocORg2ml\n",
    "stop_words = StopWordsCleaner.pretrained('stopwords_en', 'en')\\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"cleanTokens\") \\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "#https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/LemmatizerModel.html\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "         .setInputCols([\"cleanTokens\"]) \\\n",
    "         .setOutputCol(\"lemma\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCols([\"token_features\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfGrTrtZGVtN"
   },
   "outputs": [],
   "source": [
    "nlp_pipeline_lr = Pipeline(\n",
    "        stages=[document, \n",
    "                sentence,\n",
    "                token,\n",
    "                normalizer,\n",
    "                stop_words, \n",
    "                lemmatizer, \n",
    "                finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVRtMFSvGZ7b"
   },
   "source": [
    "### 4.2 Apply Pipeline to Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltwYi1mQcgv6"
   },
   "source": [
    "#### 4.2.1 Classics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf-t11ocGZhx"
   },
   "outputs": [],
   "source": [
    "processed_classics_df = nlp_pipeline_lr.fit(df_Spark_classics).transform(df_Spark_classics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLxlq30UGjg3"
   },
   "outputs": [],
   "source": [
    "#Do not execute the line below - Takes long and the output is very big.\n",
    "#Execute next cell instead, which only includes the result of interest: Token features\n",
    "\n",
    "#processed_classics_df.show(truncate=200) #Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2REFQsLHrgY",
    "outputId": "528f6e73-1310-4327-c03f-eef8a24a8799"
   },
   "outputs": [],
   "source": [
    "#Show Token Features column\n",
    "processed_classics_df.select(\"token_features\").show(truncate=200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqE2UdOXckMi"
   },
   "source": [
    "#### 4.2.2 Fanfictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wp76EXcM75Ug"
   },
   "outputs": [],
   "source": [
    "processed_fanfictions_df = nlp_pipeline_lr.fit(df_Spark_fanfictions).transform(df_Spark_fanfictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAA3y6207_v1",
    "outputId": "29e70238-742f-4189-bdf1-aac7f4fb916b"
   },
   "outputs": [],
   "source": [
    "processed_fanfictions_df.select(\"token_features\").show(truncate=200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6AFS8-N8c1v"
   },
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lU_z5wJ38f5e"
   },
   "source": [
    "At this point we have two preprocessed Spark Dataframes where each row belongs two one book. The column of interest is **\"token_features\"**, which contains all the **tokens** of the corpus.\n",
    "\n",
    "1.   **processed_classics_df**: Contains the eiight classics (one per row)\n",
    "2.   **processed_fanfictions_df**: Contains the eiight fanfictions (one per row)\n",
    "\n",
    "--> From here, we can start doing additional processing like TF-IDF or other stuff to obtain the information we want.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "br3PXqMvDfy-",
    "Q7mjzu90Ybqw",
    "mp2beC4FYesx"
   ],
   "name": "DC_Project-ClassicsVsFanfiction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

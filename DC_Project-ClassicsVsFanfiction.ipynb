{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DC_Project-ClassicsVsFanfiction.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Distributed Computing - Project 1\n","\n","\n","\n"],"metadata":{"id":"Bjz-4n_vDMl9"}},{"cell_type":"markdown","source":["## 1.Prepare Environment"],"metadata":{"id":"br3PXqMvDfy-"}},{"cell_type":"markdown","source":["### 1.1 Install Java, Pyspark and Spark NLP"],"metadata":{"id":"Q7mjzu90Ybqw"}},{"cell_type":"code","source":["import os\n","\n","!apt-get update -qq\n","!apt-get install -y openjdk-8-jdk-headless -qq\n","\n","#Install Java\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! java -version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"teXvoXAvDZY0","outputId":"e03443d8-d8b1-4afa-a71d-4862ef5d244b","executionInfo":{"status":"ok","timestamp":1649190605330,"user_tz":240,"elapsed":11323,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["openjdk version \"1.8.0_312\"\n","OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\n","OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n"]}]},{"cell_type":"code","source":["#Install Pyspark\n","! pip install --ignore-installed pyspark==2.4.4\n","\n","#Install Spark NLP\n","! pip install --ignore-installed spark-nlp==2.6.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcGxv3YuDnx4","outputId":"929b93f7-943d-4c7b-d402-ac6c2d20b22c","executionInfo":{"status":"ok","timestamp":1649190616651,"user_tz":240,"elapsed":11330,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark==2.4.4\n","  Using cached pyspark-2.4.4-py2.py3-none-any.whl\n","Collecting py4j==0.10.7\n","  Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.7 pyspark-2.4.4\n","Collecting spark-nlp==2.6.2\n","  Using cached spark_nlp-2.6.2-py2.py3-none-any.whl (128 kB)\n","Installing collected packages: spark-nlp\n","Successfully installed spark-nlp-2.6.2\n"]}]},{"cell_type":"markdown","source":["###1.2 Start  Spark Session"],"metadata":{"id":"mp2beC4FYesx"}},{"cell_type":"code","source":["import sparknlp\n","spark = sparknlp.start()\n","\n","from pyspark.ml import Pipeline\n","from sparknlp.annotator import *\n","from sparknlp.common import *\n","from sparknlp.base import *"],"metadata":{"id":"TGDRKTctUzRF","executionInfo":{"status":"ok","timestamp":1649190638845,"user_tz":240,"elapsed":22210,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## 2.Get Classics Corpus"],"metadata":{"id":"qTvaEXcQU2pP"}},{"cell_type":"markdown","source":["### 2.1 Convert txt files into Python Dataframe"],"metadata":{"id":"XB-99X0IVCsH"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import re"],"metadata":{"id":"1jCVlB2AVt1c","executionInfo":{"status":"ok","timestamp":1649190639505,"user_tz":240,"elapsed":682,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["directory =\"/content/drive/MyDrive/Distributed-Computing/data/classic_literature/\" #Change according to path\n","text_type = 'C'\n","\n","classics_df = pd.DataFrame(columns=['id', 'type', 'text'])\n","\n","for filename in os.listdir(directory):\n","#filename = \"data/classic_literature/45.txt\"\n","    file_ext = os.path.basename(filename).rsplit('.',1)[1] #Get file extension\n","    if file_ext == \"txt\":\n","        with open(directory + '/' + filename, 'r') as file:\n","            text_id = os.path.basename(filename).rsplit('.',1)[0]\n","            corpus = file.read()\n","            corpus = re.sub(';', ' ', corpus)\n","            classics_df.loc[len(classics_df.index)] = [text_id, text_type, corpus]"],"metadata":{"id":"6ybHiAU2Vwz0","executionInfo":{"status":"ok","timestamp":1649190639700,"user_tz":240,"elapsed":8,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["classics_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"hMUL6ZGUV3U5","executionInfo":{"status":"ok","timestamp":1649190639952,"user_tz":240,"elapsed":258,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}},"outputId":"3f1a4621-0c05-4567-e28d-eb01f2efc476"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     id type                                               text\n","0   514    C  \\n\\n\\n\\nLITTLE WOMEN\\n\\n\\nby\\n\\nLouisa May Alc...\n","1    45    C  \\n\\n\\n\\n\\n                  ANNE OF GREEN GABL...\n","2   768    C  \\n\\n\\n\\nTranscribed from the 1910 John Murray ...\n","3  1260    C  \\n\\n\\n\\n\\nTranscribed from the 1897 Service & ...\n","4   145    C  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMiddlemarch\\n\\n\\...\n","5  1342    C  \\n\\n\\n\\n\\nPRIDE AND PREJUDICE\\n\\nBy Jane Auste...\n","6   113    C  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn Honor of Lisa Hart's ...\n","7  1905    C  \\n\\n\\n\\n\\nTHE GOVERNESS \\n\\nOR, THE LITTLE FEM..."],"text/html":["\n","  <div id=\"df-76e42335-57c4-45c9-8b22-30c831b93e78\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>type</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>514</td>\n","      <td>C</td>\n","      <td>\\n\\n\\n\\nLITTLE WOMEN\\n\\n\\nby\\n\\nLouisa May Alc...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>45</td>\n","      <td>C</td>\n","      <td>\\n\\n\\n\\n\\n                  ANNE OF GREEN GABL...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>768</td>\n","      <td>C</td>\n","      <td>\\n\\n\\n\\nTranscribed from the 1910 John Murray ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1260</td>\n","      <td>C</td>\n","      <td>\\n\\n\\n\\n\\nTranscribed from the 1897 Service &amp; ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>145</td>\n","      <td>C</td>\n","      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMiddlemarch\\n\\n\\...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1342</td>\n","      <td>C</td>\n","      <td>\\n\\n\\n\\n\\nPRIDE AND PREJUDICE\\n\\nBy Jane Auste...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>113</td>\n","      <td>C</td>\n","      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn Honor of Lisa Hart's ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1905</td>\n","      <td>C</td>\n","      <td>\\n\\n\\n\\n\\nTHE GOVERNESS \\n\\nOR, THE LITTLE FEM...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76e42335-57c4-45c9-8b22-30c831b93e78')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-76e42335-57c4-45c9-8b22-30c831b93e78 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-76e42335-57c4-45c9-8b22-30c831b93e78');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["### 2.2 Convert Python Dataframe into Spark Dataframe"],"metadata":{"id":"d4O7Ft9_VVGZ"}},{"cell_type":"code","source":["import re\n","import pyspark\n","from pyspark import SparkContext\n","from pyspark import SparkConf\n","from pyspark.sql import SQLContext\n","\n","#sc =  pyspark.SparkContext(\"local[*]\", \"Test Context\")\n","sqlContext = SQLContext(spark)"],"metadata":{"id":"eSBwrf4sVnOT","executionInfo":{"status":"ok","timestamp":1649190779540,"user_tz":240,"elapsed":164,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["df_Spark_classics = sqlContext.createDataFrame(classics_df) #Pyspark SQL dataframe"],"metadata":{"id":"puETLHbXWG-J","executionInfo":{"status":"ok","timestamp":1649190785431,"user_tz":240,"elapsed":2650,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["df_Spark_classics.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATRk8V1dXcjR","executionInfo":{"status":"ok","timestamp":1649190794739,"user_tz":240,"elapsed":5852,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}},"outputId":"21c7e9f9-aee0-436d-ce6d-ffcc9b4e7f59"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+----+--------------------+\n","|  id|type|                text|\n","+----+----+--------------------+\n","| 514|   C|\n","\n","\n","\n","LITTLE WOMEN\n","...|\n","|  45|   C|\n","\n","\n","\n","\n","            ...|\n","| 768|   C|\n","\n","\n","\n","Transcribed f...|\n","|1260|   C|\n","\n","\n","\n","\n","Transcribed ...|\n","| 145|   C|\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Mi...|\n","|1342|   C|\n","\n","\n","\n","\n","PRIDE AND PR...|\n","| 113|   C|\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","In Hon...|\n","|1905|   C|\n","\n","\n","\n","\n","THE GOVERNES...|\n","+----+----+--------------------+\n","\n"]}]},{"cell_type":"markdown","source":["## 3.Get Fanfictions Corpus"],"metadata":{"id":"7tnqsgNLVqcr"}},{"cell_type":"markdown","source":["### 3.1 Convert txt files into Python Dataframe"],"metadata":{"id":"O9AwiLtTX9Gf"}},{"cell_type":"code","source":[""],"metadata":{"id":"q_061UaIX8Wl","executionInfo":{"status":"aborted","timestamp":1649190640173,"user_tz":240,"elapsed":8,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2 Convert Python Dataframe into Spark Dataframe"],"metadata":{"id":"P_xfXM-JYCVe"}},{"cell_type":"code","source":[""],"metadata":{"id":"D2nGH71mX71g","executionInfo":{"status":"aborted","timestamp":1649190640174,"user_tz":240,"elapsed":9,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6T94RjuTDzGz","executionInfo":{"status":"aborted","timestamp":1649190640175,"user_tz":240,"elapsed":10,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4 Preprocess Texts"],"metadata":{"id":"675Exsu0Y3uL"}},{"cell_type":"markdown","source":["### 4.1 Create Preprocessing Pipeline"],"metadata":{"id":"L9m3hBQZF8SN"}},{"cell_type":"markdown","source":["Create pipeline to preprocess the spark dataframe texts.\n","\n","Each of these classes receive an input column and creates the output column.\n","At the end of the pipeline, we will have a dataframe with all of the columns that are created on the fly and their results.\n","\n","The **last column** generated, in this case **token_features** is the one that has all the words after being preprocessed, removing stop words, etc."],"metadata":{"id":"YYzZHjoXGBs3"}},{"cell_type":"code","source":["#https://medium.com/spark-nlp/spark-nlp-101-document-assembler-500018f5f6b5\n","document = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\") \\\n","    .setCleanupMode(\"shrink_full\") #remove new lines and tabs, plus shrinking spaces and blank lines.\n","\n","#We dont need this because when preprocessing, the test ends up being one sentence\n","sentence = SentenceDetector()\\\n","    .setInputCols(['document'])\\\n","    .setOutputCol('sentence')\n","\n","#https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.Tokenizer.html\n","token = Tokenizer()\\\n","    .setInputCols(['sentence'])\\\n","    .setOutputCol('token')\n","\n","#https://nlp.johnsnowlabs.com/docs/en/annotators\n","normalizer = Normalizer() \\\n","    .setInputCols([\"token\"]) \\\n","    .setOutputCol(\"normalized\") \\\n","    .setLowercase(True) \\\n","    .setCleanupPatterns([\"\"\"[^A-Za-z]\"\"\"]) # remove punctuations and alphanumeric chars\n","\n","#Stop words used by Spark NLP: http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n","#https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/b2eb08610dd49d5b15077cc499a94b4ec1e8b861/jupyter/annotation/english/stop-words/StopWordsCleaner.ipynb#scrollTo=1-eGocORg2ml\n","stop_words = StopWordsCleaner.pretrained('stopwords_en', 'en')\\\n","    .setInputCols([\"normalized\"]) \\\n","    .setOutputCol(\"cleanTokens\") \\\n","    .setCaseSensitive(False)\n","\n","#https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/LemmatizerModel.html\n","lemmatizer = LemmatizerModel.pretrained() \\\n","         .setInputCols([\"cleanTokens\"]) \\\n","         .setOutputCol(\"lemma\")\n","\n","finisher = Finisher() \\\n","    .setInputCols([\"lemma\"]) \\\n","    .setOutputCols([\"token_features\"]) \\\n","    .setOutputAsArray(True) \\\n","    .setCleanAnnotations(False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pRf9LnvgGQwi","outputId":"0b036600-6901-4eef-c257-2563a42bc8de","executionInfo":{"status":"ok","timestamp":1649190812295,"user_tz":240,"elapsed":11356,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["stopwords_en download started this may take some time.\n","Approximate size to download 2.9 KB\n","[OK!]\n","lemma_antbnc download started this may take some time.\n","Approximate size to download 907.6 KB\n","[OK!]\n"]}]},{"cell_type":"code","source":["nlp_pipeline_lr = Pipeline(\n","        stages=[document, \n","                sentence,\n","                token,\n","                normalizer,\n","                stop_words, \n","                lemmatizer, \n","                finisher])"],"metadata":{"id":"WfGrTrtZGVtN","executionInfo":{"status":"ok","timestamp":1649190816662,"user_tz":240,"elapsed":209,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### 4.2 Apply Pipeline to Spark Dataframes"],"metadata":{"id":"qVRtMFSvGZ7b"}},{"cell_type":"markdown","source":["#### 4.2.1 Classics"],"metadata":{"id":"ltwYi1mQcgv6"}},{"cell_type":"code","source":["processed_text = nlp_pipeline_lr.fit(df_Spark_classics).transform(df_Spark_classics)"],"metadata":{"id":"Hf-t11ocGZhx","executionInfo":{"status":"ok","timestamp":1649190843428,"user_tz":240,"elapsed":1705,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["#Do not execute the line below - Takes long and the output is very big.\n","#Execute next cell instead, which only includes the result of interest: Token features\n","\n","#processed_text.show(truncate=200) #Show results"],"metadata":{"id":"tLxlq30UGjg3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Show Token Features column\n","processed_text.select(\"token_features\").show(truncate=200) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2REFQsLHrgY","outputId":"0a47d6ca-ef7c-42fe-ec24-f62911dd980f","executionInfo":{"status":"ok","timestamp":1649191188847,"user_tz":240,"elapsed":41153,"user":{"displayName":"Marina SanchezMillan","userId":"03581886952206704676"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|                                                                                                                                                                                          token_features|\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|[woman, louisa, alcott, content, part, play, pilgrim, merry, christmas, laurence, boy, burden, neighborly, beth, find, palace, beautiful, amy, valley, humiliation, jo, meet, apollyon, meg, vanity, ...|\n","|[anne, green, gable, lucy, maud, montgomery, table, content, chapter, mrs, rachel, lynde, surprise, chapter, ii, matthew, cuthbert, surprise, chapter, iii, marilla, cuthbert, surprise, chapter, iv,...|\n","|[transcribe, john, murray, edition, david, price, email, ccxpglaforg, wuthering, height, chapter, return, visit, landlordthe, solitary, neighbour, trouble, beautiful, country, england, fix, situati...|\n","|[transcribe, service, paton, edition, david, price, email, ccxpglaforg, jane, eyre, autobiography, charlotte, bronte, illustrate, townsend, london, service, paton, henrietta, street, illustration, ...|\n","|[middlemarch, george, eliot, york, boston, caldwell, company, publisher, dear, husband, george, henry, lewes, nineteenth, year, bless, union, content, book, chapter, chapter, ii, chapter, iii, chap...|\n","|[pride, prejudice, jane, austen, chapter, truth, universally, acknowledge, single, man, possession, good, fortune, wife, feelings, view, man, enter, neighbourhood, truth, fix, mind, surround, famil...|\n","|[honor, lisa, hart, birthday, secret, garden, frances, hodgson, burnett, author, shuttle, make, marchioness, method, lady, walderhurst, lass, lowries, administration, lord, fauntleroy, lady, qualit...|\n","|[governess, female, academy, sarah, field, live, northern, part, england, gentlewoman, undertake, education, young, lady, trust, endeavour, faithfully, discharge, instruct, commit, care, read, writ...|\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["#### 4.2.2 Fanfictions"],"metadata":{"id":"XqE2UdOXckMi"}}]}
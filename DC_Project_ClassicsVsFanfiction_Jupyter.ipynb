{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjz-4n_vDMl9"
   },
   "source": [
    "# Distributed Computing - Project 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br3PXqMvDfy-"
   },
   "source": [
    "## 1.Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7mjzu90Ybqw"
   },
   "source": [
    "### 1.1 Install Java, Pyspark and Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "teXvoXAvDZY0",
    "outputId": "f4dc801d-7b3d-4363-bdd6-3bd0cc7cf940"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update -qq\n",
    "#!apt-get install -y openjdk-8-jdk-headless -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_312\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~20.04-b07)\n",
      "OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "#Install Java\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TcGxv3YuDnx4",
    "outputId": "2555c265-1675-4ce2-9c41-c655466c0ed5"
   },
   "outputs": [],
   "source": [
    "#Install Pyspark\n",
    "#! pip install --ignore-installed pyspark==2.4.4\n",
    "\n",
    "#Install Spark NLP\n",
    "#! pip install --ignore-installed spark-nlp==2.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp2beC4FYesx"
   },
   "source": [
    "### 1.2 Start  Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGDRKTctUzRF"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "#spark = sparknlp.start()\n",
    "spark = sparknlp.start(spark32=True)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTvaEXcQU2pP"
   },
   "source": [
    "## 2.Get Classics Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB-99X0IVCsH"
   },
   "source": [
    "### 2.1 Convert txt files into Python Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1jCVlB2AVt1c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6ybHiAU2Vwz0"
   },
   "outputs": [],
   "source": [
    "#directory =\"/content/drive/MyDrive/Distributed-Computing/data/classic_literature/\" #Change according to path\n",
    "directory =\"data/classic_literature/\" #Change according to path\n",
    "text_type = 'C'\n",
    "\n",
    "classics_df = pd.DataFrame(columns=['id', 'type', 'text'])\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "#filename = \"data/classic_literature/45.txt\"\n",
    "    file_ext = os.path.basename(filename).rsplit('.',1)[1] #Get file extension\n",
    "    if file_ext == \"txt\":\n",
    "        with open(directory + '/' + filename, 'r') as file:\n",
    "            text_id = os.path.basename(filename).rsplit('.',1)[0]\n",
    "            corpus = file.read()\n",
    "            corpus = re.sub(';', ' ', corpus)\n",
    "            corpus = corpus.replace('Chapter', '')\n",
    "            classics_df.loc[len(classics_df.index)] = [text_id, text_type, corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "hMUL6ZGUV3U5",
    "outputId": "164d8c67-900f-47fd-cdbd-6a6d01fbe87b"
   },
   "outputs": [],
   "source": [
    "classics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4O7Ft9_VVGZ"
   },
   "source": [
    "### 2.2 Convert Python Dataframe into Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eSBwrf4sVnOT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#sc =  pyspark.SparkContext(\"local[*]\", \"Test Context\")\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "puETLHbXWG-J"
   },
   "outputs": [],
   "source": [
    "df_Spark_classics = sqlContext.createDataFrame(classics_df) #Pyspark SQL dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATRk8V1dXcjR"
   },
   "outputs": [],
   "source": [
    "#df_Spark_classics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tnqsgNLVqcr"
   },
   "source": [
    "## 3.Get Fanfictions Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9AwiLtTX9Gf"
   },
   "source": [
    "### 3.1 Convert txt files into Python Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_061UaIX8Wl"
   },
   "outputs": [],
   "source": [
    "#directory =\"/content/drive/MyDrive/Distributed-Computing/data/fanfiction/\" #Change according to path\n",
    "directory =\"data/fanfiction/\" #Change according to path\n",
    "text_type = 'F'\n",
    "\n",
    "fanfictions_df = pd.DataFrame(columns=['id', 'type', 'text'])\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_ext = os.path.basename(filename).rsplit('.',1)[1] #Get file extension\n",
    "    if file_ext == \"txt\":\n",
    "        with open(directory + '/' + filename, 'r') as file:\n",
    "            text_id = os.path.basename(filename).rsplit('.',1)[0]\n",
    "            corpus = file.read()\n",
    "            corpus = corpus.replace('Chapter', '')\n",
    "            corpus = re.sub(';', ' ', corpus)\n",
    "            fanfictions_df.loc[len(fanfictions_df.index)] = [text_id, text_type, corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "iZ2UUw7D7QKt",
    "outputId": "819eb55a-1b3d-41b9-a469-decf732962e8"
   },
   "outputs": [],
   "source": [
    "fanfictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_xfXM-JYCVe"
   },
   "source": [
    "### 3.2 Convert Python Dataframe into Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2nGH71mX71g"
   },
   "outputs": [],
   "source": [
    "df_Spark_fanfictions = sqlContext.createDataFrame(fanfictions_df) #Pyspark SQL dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "675Exsu0Y3uL"
   },
   "source": [
    "## 4 Preprocess Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9m3hBQZF8SN"
   },
   "source": [
    "### 4.1 Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYzZHjoXGBs3"
   },
   "source": [
    "Create pipeline to preprocess the spark dataframe texts.\n",
    "\n",
    "Each of these classes receive an input column and creates the output column.\n",
    "At the end of the pipeline, we will have a dataframe with all of the columns that are created on the fly and their results.\n",
    "\n",
    "The **last column** generated, in this case **token_features** is the one that has all the words after being preprocessed, removing stop words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRf9LnvgGQwi",
    "outputId": "17982f91-18cb-48db-a01b-90829528f1c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[ | ]stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "#https://medium.com/spark-nlp/spark-nlp-101-document-assembler-500018f5f6b5\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink_full\") #remove new lines and tabs, plus shrinking spaces and blank lines.\n",
    "\n",
    "#We dont need this because when preprocessing, the test ends up being one sentence\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "#https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.Tokenizer.html\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "#https://nlp.johnsnowlabs.com/docs/en/annotators\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^A-Za-z]\"\"\"]) # remove punctuations and alphanumeric chars\n",
    "\n",
    "#Stop words used by Spark NLP: http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n",
    "#https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/b2eb08610dd49d5b15077cc499a94b4ec1e8b861/jupyter/annotation/english/stop-words/StopWordsCleaner.ipynb#scrollTo=1-eGocORg2ml\n",
    "stop_words = StopWordsCleaner.pretrained('stopwords_en', 'en')\\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"cleanTokens\") \\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "#https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/LemmatizerModel.html\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "         .setInputCols([\"cleanTokens\"]) \\\n",
    "         .setOutputCol(\"lemma\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCols([\"token_features\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WfGrTrtZGVtN"
   },
   "outputs": [],
   "source": [
    "nlp_pipeline_lr = Pipeline(\n",
    "        stages=[document, \n",
    "                sentence,\n",
    "                token,\n",
    "                normalizer,\n",
    "                stop_words, \n",
    "                lemmatizer, \n",
    "                finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVRtMFSvGZ7b"
   },
   "source": [
    "### 4.2 Apply Pipeline to Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltwYi1mQcgv6"
   },
   "source": [
    "#### 4.2.1 Classics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf-t11ocGZhx"
   },
   "outputs": [],
   "source": [
    "processed_classics_df = nlp_pipeline_lr.fit(df_Spark_classics).transform(df_Spark_classics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLxlq30UGjg3"
   },
   "outputs": [],
   "source": [
    "#Do not execute the line below - Takes long and the output is very big.\n",
    "#Execute next cell instead, which only includes the result of interest: Token features\n",
    "\n",
    "#processed_classics_df.show(truncate=200) #Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2REFQsLHrgY",
    "outputId": "528f6e73-1310-4327-c03f-eef8a24a8799"
   },
   "outputs": [],
   "source": [
    "#Show Token Features column\n",
    "processed_classics_df.select(\"token_features\").show(truncate=200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqE2UdOXckMi"
   },
   "source": [
    "#### 4.2.2 Fanfictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wp76EXcM75Ug"
   },
   "outputs": [],
   "source": [
    "processed_fanfictions_df = nlp_pipeline_lr.fit(df_Spark_fanfictions).transform(df_Spark_fanfictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAA3y6207_v1",
    "outputId": "29e70238-742f-4189-bdf1-aac7f4fb916b"
   },
   "outputs": [],
   "source": [
    "processed_fanfictions_df.select(\"token_features\").show(truncate=200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6AFS8-N8c1v"
   },
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lU_z5wJ38f5e"
   },
   "source": [
    "At this point we have two preprocessed Spark Dataframes where each row belongs two one book. The column of interest is **\"token_features\"**, which contains all the **tokens** of the corpus.\n",
    "\n",
    "1.   **processed_classics_df**: Contains the eiight classics (one per row)\n",
    "2.   **processed_fanfictions_df**: Contains the eiight fanfictions (one per row)\n",
    "\n",
    "--> From here, we can start doing additional processing like TF-IDF or other stuff to obtain the information we want.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "br3PXqMvDfy-",
    "Q7mjzu90Ybqw",
    "mp2beC4FYesx"
   ],
   "name": "DC_Project-ClassicsVsFanfiction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

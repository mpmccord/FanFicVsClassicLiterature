{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effe7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f78f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 10:05:02 WARN Utils: Your hostname, DSGPU05 resolves to a loopback address: 127.0.1.1; using 10.10.11.64 instead (on interface eno1)\n",
      "22/04/15 10:05:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/15 10:05:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import pyspark.sql.functions as f\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe2736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.appName('App Name').master(\"local[*]\").getOrCreate()\n",
    "sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b680e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "classics = sqlContext.read.format(\"csv\") \\\n",
    "   .options(header='true', inferschema='true') \\\n",
    "   .load(os.path.realpath(\"data/fanfics_raw.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2858d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|                text|                type|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|             1536152|           Chapter 1|                null|\n",
      "|Going back was th...| after a case lik...| I’d be moved to ...|\n",
      "|           Chapter 2|                null|                null|\n",
      "|It was during my ...| Sophia was no lo...| especially since...|\n",
      "|           Chapter 3|                null|                null|\n",
      "|Following Sophia ...| I ducked in an e...| waiting under a ...|\n",
      "|           Chapter 4|                null|                null|\n",
      "|Aegis breaking in...| but when they ca...| barely managing ...|\n",
      "|           Chapter 5|                null|                null|\n",
      "|I hated coming to...|                sure| but everyone her...|\n",
      "|           Chapter 6|                null|                null|\n",
      "|             Somehow| this time was ev...| my instincts wer...|\n",
      "|           Chapter 7|                null|                null|\n",
      "|The restaurant Li...| just off the Boa...| and without thos...|\n",
      "|           Chapter 8|                null|                null|\n",
      "|I managed to snea...| I tested for spe...| there were shado...|\n",
      "|           Chapter 9|                null|                null|\n",
      "|My alarm woke me ...| wondering why I ...|    the infiltration|\n",
      "|          Chapter 10|                null|                null|\n",
      "|I wasn't ready fo...| when the bullyin...| but it had been ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4efdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696dccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bce559ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                  id|                text|                type|                line|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|             1536152|           Chapter 1|                null|           Chapter 1|\n",
      "|Going back was th...| after a case lik...| I’d be moved to ...| after a case lik...|\n",
      "|It was during my ...| Sophia was no lo...| especially since...| Sophia was no lo...|\n",
      "|Following Sophia ...| I ducked in an e...| waiting under a ...| I ducked in an e...|\n",
      "|Aegis breaking in...| but when they ca...| barely managing ...| but when they ca...|\n",
      "|I hated coming to...|                sure| but everyone her...|                sure|\n",
      "|             Somehow| this time was ev...| my instincts wer...| this time was ev...|\n",
      "|The restaurant Li...| just off the Boa...| and without thos...| just off the Boa...|\n",
      "|I managed to snea...| I tested for spe...| there were shado...| I tested for spe...|\n",
      "|My alarm woke me ...| wondering why I ...|    the infiltration| wondering why I ...|\n",
      "|I wasn't ready fo...| when the bullyin...| but it had been ...| when the bullyin...|\n",
      "|It had been somew...| on making her laugh| and I could keep...| on making her laugh|\n",
      "|It took me a week...| sometimes as lat...| which made getti...| sometimes as lat...|\n",
      "|It was four days ...| I simply rested....|         no research| I simply rested....|\n",
      "|I felt a smile ri...| I realized. Talk...| I added sarcasti...| I realized. Talk...|\n",
      "|As soon as I fini...| I went straight ...| but once there I...| I went straight ...|\n",
      "|“Almost have it…”...| my voice echoing...| and heard a slig...| my voice echoing...|\n",
      "|I reacted instinc...| turning into sha...| there was still ...| turning into sha...|\n",
      "|I sent a message ...| so shadow state ...| so I had hope th...| so shadow state ...|\n",
      "|On my way… Thank god| I said to myself...| the both of them...| I said to myself...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classics_df = classics.withColumn('line', f.explode(f.split(f.col('text'), '\\n')))\n",
    "classics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086ae38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classics = classics_df.rdd.map(lambda x : x['line']).filter(lambda x: x is not None)\n",
    "StopWords = stopwords.words(\"english\")\n",
    "tokens = classics                                                  \\\n",
    "    .map( lambda document: document.strip().lower())               \\\n",
    "    .map( lambda document: re.split(\" \", document))          \\\n",
    "    .map( lambda word: [x for x in word if x.isalpha()])           \\\n",
    "    .map( lambda word: [x for x in word if len(x) > 3] )           \\\n",
    "    .map( lambda word: [x for x in word if x not in StopWords])    \\\n",
    "    .zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92419692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, StringType, column\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "df_txts = sqlContext.createDataFrame(tokens, [\"list_of_words\",'index'])# TF\n",
    "cv = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"raw_features\", vocabSize=5000, minDF=1.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "result_cv = cvmodel.transform(df_txts)\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aefa2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, StringType, column\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "df_txts = sqlContext.createDataFrame(tokens, [\"list_of_words\",'index'])# TF\n",
    "cv = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"raw_features\", vocabSize=5000, minDF=1.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "result_cv = cvmodel.transform(df_txts)\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2190d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 10:05:19 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/04/15 10:05:19 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "num_topics = 10\n",
    "max_iterations = 100\n",
    "lda = LDA(k=num_topics, maxIter=max_iterations)\n",
    "lda_model = lda.fit(result_tfidf[['index','features']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f13f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- line: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                  id|                text|                type|                line|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|             1536152|           Chapter 1|                null|           Chapter 1|\n",
      "|Going back was th...| after a case lik...| I’d be moved to ...| after a case lik...|\n",
      "|It was during my ...| Sophia was no lo...| especially since...| Sophia was no lo...|\n",
      "|Following Sophia ...| I ducked in an e...| waiting under a ...| I ducked in an e...|\n",
      "|Aegis breaking in...| but when they ca...| barely managing ...| but when they ca...|\n",
      "|I hated coming to...|                sure| but everyone her...|                sure|\n",
      "|             Somehow| this time was ev...| my instincts wer...| this time was ev...|\n",
      "|The restaurant Li...| just off the Boa...| and without thos...| just off the Boa...|\n",
      "|I managed to snea...| I tested for spe...| there were shado...| I tested for spe...|\n",
      "|My alarm woke me ...| wondering why I ...|    the infiltration| wondering why I ...|\n",
      "|I wasn't ready fo...| when the bullyin...| but it had been ...| when the bullyin...|\n",
      "|It had been somew...| on making her laugh| and I could keep...| on making her laugh|\n",
      "|It took me a week...| sometimes as lat...| which made getti...| sometimes as lat...|\n",
      "|It was four days ...| I simply rested....|         no research| I simply rested....|\n",
      "|I felt a smile ri...| I realized. Talk...| I added sarcasti...| I realized. Talk...|\n",
      "|As soon as I fini...| I went straight ...| but once there I...| I went straight ...|\n",
      "|“Almost have it…”...| my voice echoing...| and heard a slig...| my voice echoing...|\n",
      "|I reacted instinc...| turning into sha...| there was still ...| turning into sha...|\n",
      "|I sent a message ...| so shadow state ...| so I had hope th...| so shadow state ...|\n",
      "|On my way… Thank god| I said to myself...| the both of them...| I said to myself...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classics_df.printSchema()\n",
    "classics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb1f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[14, 139, 189, 17...|[0.02141250317791...|\n",
      "|    1|[66, 28, 31, 113,...|[0.01756785798549...|\n",
      "|    2|[5, 38, 7, 56, 24...|[0.02257030944954...|\n",
      "|    3|[30, 33, 37, 44, ...|[0.02118311042827...|\n",
      "|    4|[23, 32, 51, 47, ...|[0.02362122768140...|\n",
      "|    5|[3, 16, 10, 17, 2...|[0.04733502409303...|\n",
      "|    6|[20, 19, 71, 65, ...|[0.02669326733037...|\n",
      "|    7|[11, 39, 29, 1, 2...|[0.02726499944641...|\n",
      "|    8|[35, 107, 59, 146...|[0.02257721499493...|\n",
      "|    9|[0, 22, 48, 82, 6...|[0.04670586419601...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n"
     ]
    }
   ],
   "source": [
    "lda_model.describeTopics().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b26c31fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.sparkContext.broadcast(vocab)\n",
    "\n",
    "#creating LDA model\n",
    "ldatopics = lda_model.describeTopics()\n",
    "\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "\n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f660c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|          topic_desc|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    0|[14, 139, 189, 17...|[0.02141250317791...|[really, feeling,...|\n",
      "|    1|[66, 28, 31, 113,...|[0.01756785798549...|[another, eyes, f...|\n",
      "|    2|[5, 38, 7, 56, 24...|[0.02257030944954...|[know, though, ev...|\n",
      "|    3|[30, 33, 37, 44, ...|[0.02118311042827...|[chloe, looking, ...|\n",
      "|    4|[23, 32, 51, 47, ...|[0.02362122768140...|[make, sure, alre...|\n",
      "|    5|[3, 16, 10, 17, 2...|[0.04733502409303...|[says, beca, quin...|\n",
      "|    6|[20, 19, 71, 65, ...|[0.02669326733037...|[elizabeth, still...|\n",
      "|    7|[11, 39, 29, 1, 2...|[0.02726499944641...|[think, well, rig...|\n",
      "|    8|[35, 107, 59, 146...|[0.02257721499493...|[look, turned, ha...|\n",
      "|    9|[0, 22, 48, 82, 6...|[0.04670586419601...|[said, darcy, nee...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ldatopics_mapped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dcdbf6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Resolved attribute(s) topic_desc#190 missing from topic#183,termIndices#184,termWeights#185,topic_desc#214 in operator !Project [topic#183, termIndices#184, concat_ws( , topic_desc#190) AS termWeights#220, topic_desc#214]. Attribute(s) with the same name appear in the operation: topic_desc. Please check if the right attribute(s) are used.;\n!Project [topic#183, termIndices#184, concat_ws( , topic_desc#190) AS termWeights#220, topic_desc#214]\n+- Project [topic#183, termIndices#184, termWeights#185, concat_ws( , topic_desc#190) AS topic_desc#214]\n   +- Project [topic#183, termIndices#184, termWeights#185, map_termID_to_Word(termIndices#184) AS topic_desc#190]\n      +- Project [_1#177 AS topic#183, _2#178 AS termIndices#184, _3#179 AS termWeights#185]\n         +- LocalRelation [_1#177, _2#178, _3#179]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m lda_topics_mapped \u001b[38;5;241m=\u001b[39m ldatopics_mapped\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_desc\u001b[39m\u001b[38;5;124m\"\u001b[39m, concat_ws(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, ldatopics_mapped\u001b[38;5;241m.\u001b[39mtopic_desc)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_desc_str\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m lda_topics_mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlda_topics_mapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtermWeights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_ws\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mldatopics_mapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopic_desc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic_desc_str\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m lda_topics_mapped\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:2478\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1256\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1250\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1252\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1253\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1255\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1256\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1260\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Resolved attribute(s) topic_desc#190 missing from topic#183,termIndices#184,termWeights#185,topic_desc#214 in operator !Project [topic#183, termIndices#184, concat_ws( , topic_desc#190) AS termWeights#220, topic_desc#214]. Attribute(s) with the same name appear in the operation: topic_desc. Please check if the right attribute(s) are used.;\n!Project [topic#183, termIndices#184, concat_ws( , topic_desc#190) AS termWeights#220, topic_desc#214]\n+- Project [topic#183, termIndices#184, termWeights#185, concat_ws( , topic_desc#190) AS topic_desc#214]\n   +- Project [topic#183, termIndices#184, termWeights#185, map_termID_to_Word(termIndices#184) AS topic_desc#190]\n      +- Project [_1#177 AS topic#183, _2#178 AS termIndices#184, _3#179 AS termWeights#185]\n         +- LocalRelation [_1#177, _2#178, _3#179]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, split\n",
    "import pyspark.sql.functions as f\n",
    "lda_topics_mapped = ldatopics_mapped.withColumn(\"topic_desc\", concat_ws(\" \", ldatopics_mapped.topic_desc).alias(\"topic_desc_str\"))\n",
    "lda_topics_mapped = lda_topics_mapped.withColumn(\"termWeights\", concat_ws(\" \", ldatopics_mapped.topic_desc).alias(\"topic_desc_str\"))\n",
    "lda_topics_mapped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f75822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ldatopics_mapped.withColumn(\"new\", f.arrays_zip(\"topic_desc\", \"termWeights\"))\\\n",
    "       .withColumn(\"new\", f.explode(\"new\")) \\\n",
    "        .select(\"topic\", f.col(\"new.topic_desc\").alias(\"words\"), f.col(\"new.termWeights\").alias(\"termWeights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe35897",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1180b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from collections import ChainMap\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(background_color=\"white\")\n",
    "\n",
    "words = dict(ChainMap(*df.select(F.create_map('words', 'termWeights')).rdd.map(lambda x: x[0]).collect()))\n",
    "# {'scorbutically': 1.76, 'punta': 1.76, 'detail': 1.789, 'lafayette': 1.8, 'maya': 1.854, 'prostate': 1.854, 'quot': 1.856, 'mark': 1.949, 'elite': 1.988, 'trade': 2.012, 'write': 2.083}\n",
    "\n",
    "plt.imshow(wordcloud.generate_from_frequencies(words))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8b632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ecee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

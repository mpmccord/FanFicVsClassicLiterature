{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f4a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a7b830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 19:42:46 WARN Utils: Your hostname, DSGPU05 resolves to a loopback address: 127.0.1.1; using 10.10.11.64 instead (on interface eno1)\n",
      "22/04/12 19:42:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/12 19:42:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/12 19:42:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/04/12 19:42:48 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/04/12 19:42:48 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import pyspark.sql.functions as f\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e206ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.appName('App Name').master(\"local[*]\").getOrCreate()\n",
    "sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10cd61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sqlContext.read.format(\"csv\") \\\n",
    "   .options(header='true', inferschema='true') \\\n",
    "   .load(os.path.realpath(\"data/fanfics_raw.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9fe8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Arthur', 'NNP'), ('loves', 'VBZ'), ('fish', 'JJ')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.pos_tag([\"Arthur\", \"loves\", \"fish\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e862e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                  id|                text|                type|                line|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|             1536152|           Chapter 1|                null|           Chapter 1|\n",
      "|Going back was th...| after a case lik...| I’d be moved to ...| after a case lik...|\n",
      "|It was during my ...| Sophia was no lo...| especially since...| Sophia was no lo...|\n",
      "|Following Sophia ...| I ducked in an e...| waiting under a ...| I ducked in an e...|\n",
      "|Aegis breaking in...| but when they ca...| barely managing ...| but when they ca...|\n",
      "|I hated coming to...|                sure| but everyone her...|                sure|\n",
      "|             Somehow| this time was ev...| my instincts wer...| this time was ev...|\n",
      "|The restaurant Li...| just off the Boa...| and without thos...| just off the Boa...|\n",
      "|I managed to snea...| I tested for spe...| there were shado...| I tested for spe...|\n",
      "|My alarm woke me ...| wondering why I ...|    the infiltration| wondering why I ...|\n",
      "|I wasn't ready fo...| when the bullyin...| but it had been ...| when the bullyin...|\n",
      "|It had been somew...| on making her laugh| and I could keep...| on making her laugh|\n",
      "|It took me a week...| sometimes as lat...| which made getti...| sometimes as lat...|\n",
      "|It was four days ...| I simply rested....|         no research| I simply rested....|\n",
      "|I felt a smile ri...| I realized. Talk...| I added sarcasti...| I realized. Talk...|\n",
      "|As soon as I fini...| I went straight ...| but once there I...| I went straight ...|\n",
      "|“Almost have it…”...| my voice echoing...| and heard a slig...| my voice echoing...|\n",
      "|I reacted instinc...| turning into sha...| there was still ...| turning into sha...|\n",
      "|I sent a message ...| so shadow state ...| so I had hope th...| so shadow state ...|\n",
      "|On my way… Thank god| I said to myself...| the both of them...| I said to myself...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "data = data.withColumn('line', f.explode(f.split(f.col('text'), '\\n')))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8790e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeProperNouns(sentence):\n",
    "    tagged_sentence = nltk.tag.pos_tag(nltk.word_tokenize(sentence))\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    return ' '.join(edited_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63277344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f636a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "classics = sqlContext.read.format(\"csv\") \\\n",
    "   .options(header='true', inferschema='true') \\\n",
    "   .load(os.path.realpath(\"data/classics_raw.csv\"))\n",
    "StopWords = nltk.corpus.stopwords.words(\"english\")\n",
    "classics = data.rdd.map(lambda x : x['text']).filter(lambda x: x is not None)\n",
    "tokens = classics                                                 \\\n",
    "    .map(lambda document: removeProperNouns(document))             \\\n",
    "    .map( lambda document: document.strip().lower())               \\\n",
    "    .map( lambda document: re.split(\" \", document))          \\\n",
    "    .map( lambda word: [x for x in word if x.isalpha()])           \\\n",
    "    .map( lambda word: [x for x in word if len(x) > 3] )           \\\n",
    "    .map( lambda word: [x for x in word if x not in StopWords])     \\\n",
    "    .zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52cfb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, StringType, column\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "df_txts = sqlContext.createDataFrame(tokens, [\"list_of_words\",'index'])# TF\n",
    "cv = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"raw_features\", vocabSize=5000, minDF=1.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "result_cv = cvmodel.transform(df_txts)\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, StringType, column\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "df_txts = sqlContext.createDataFrame(tokens, [\"list_of_words\",'index'])# TF\n",
    "cv = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"raw_features\", vocabSize=5000, minDF=1.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "result_cv = cvmodel.transform(df_txts)\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "max_iterations = 1000\n",
    "lda = LDA(k=num_topics, maxIter=max_iterations)\n",
    "lda_model = lda.fit(result_tfidf[['index','features']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa68848",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781578f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.sparkContext.broadcast(vocab)\n",
    "\n",
    "#creating LDA model\n",
    "ldatopics = lda_model.describeTopics()\n",
    "\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "\n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c2d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldatopics_mapped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, split\n",
    "import pyspark.sql.functions as f\n",
    "lda_topics_mapped = ldatopics_mapped.withColumn(\"topic_desc\", concat_ws(\" \", ldatopics_mapped.topic_desc).alias(\"topic_desc_str\"))\n",
    "\n",
    "lda_topics_mapped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ca6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = lda_topics_mapped.withColumn('word', f.explode(f.split(f.col('topic_desc'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=True)\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1df573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from collections import ChainMap\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(background_color=\"white\")\n",
    "\n",
    "words = dict(ChainMap(*counts.select(F.create_map('word', 'count')).rdd.map(lambda x: x[0]).collect()))\n",
    "# {'scorbutically': 1.76, 'punta': 1.76, 'detail': 1.789, 'lafayette': 1.8, 'maya': 1.854, 'prostate': 1.854, 'quot': 1.856, 'mark': 1.949, 'elite': 1.988, 'trade': 2.012, 'write': 2.083}\n",
    "\n",
    "plt.imshow(wordcloud.generate_from_frequencies(words))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fea407",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag([\"lisa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34920c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "removeProperNouns(\"Lisa is me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce8ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
